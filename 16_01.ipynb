{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66f0360e",
   "metadata": {},
   "source": [
    "# 模型构造"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1b297c",
   "metadata": {},
   "source": [
    "## 层和块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1091b82",
   "metadata": {},
   "source": [
    "## 回顾一下多层感知机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bac63241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.5866e-01,  8.3700e-02, -8.0222e-02,  2.0417e-01, -5.9994e-02,\n",
       "         -7.6787e-02, -1.4708e-01, -1.8244e-01, -1.4637e-01,  2.3431e-01],\n",
       "        [-2.3706e-01, -6.9505e-03, -1.0128e-01,  9.1590e-02, -9.6492e-05,\n",
       "         -1.1386e-01, -1.5017e-01, -1.5856e-01, -2.0885e-01,  2.3384e-01]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#回顾一下多层感知机\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "#nn.Sequential定义了一个特殊的Module\n",
    "net=nn.Sequential(nn.Linear(20,256),nn.ReLU(),nn.Linear(256,10))\n",
    "\n",
    "#随机生成一个2*20的矩阵，满足均匀分布\n",
    "X=torch.rand(2,20)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dd6470",
   "metadata": {},
   "source": [
    "## 自定义MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10fa4db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0325,  0.0237, -0.0557, -0.1316,  0.1018, -0.2487, -0.0263,  0.0706,\n",
       "         -0.1457, -0.1961],\n",
       "        [ 0.0813, -0.0072,  0.0048, -0.0931,  0.1010, -0.3355,  0.0130,  0.0275,\n",
       "         -0.0447, -0.2847]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#任何一个层和神经网络都是Module的子类\n",
    "#自定义一个MLP\n",
    "#MLP就可以继承Module中很多的函数\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    #这个函数就用来定义一些参数和类\n",
    "    #网络中调用的所有层都在这个函数里面\n",
    "    def __init__(self):\n",
    "        #调用父类，把一些内部参数全部设置好，这样在初始化weight可以全部弄好\n",
    "        super().__init__()\n",
    "        self.hidden=nn.Linear(20,256)\n",
    "        self.out=nn.Linear(256,10)\n",
    "    #前向计算\n",
    "    def forward(self,X):\n",
    "        return self.out(F.relu(self.hidden(X)))\n",
    "\n",
    "#实例化多层感知机的层，每次调用正向传播函数时调用这些层\n",
    "net=MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fb5d89",
   "metadata": {},
   "source": [
    "## 实现nn.Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79d7701",
   "metadata": {},
   "source": [
    "#实现nn.Sequential\n",
    "class MySequential(nn.Module):\n",
    "    def __init__(self,*args):#args里面存放各个层\n",
    "        super().__init__()\n",
    "        for block in args:\n",
    "            #做成一个字典\n",
    "            self._modules[block]=block\n",
    "            \n",
    "    def forward(self,X):\n",
    "        #把各个层取出来，输入X得到一个输出，不断按顺序经过每一层\n",
    "        for block in self._modules.values():\n",
    "            X=block(X)\n",
    "            #print(self._modules)\n",
    "            #print(self._modules.values())\n",
    "        return X\n",
    "    \n",
    "net=MySequential(nn.Linear(20,256),nn.ReLU(),nn.Linear(256,10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5271c389",
   "metadata": {},
   "source": [
    "## 在正向传播函数中执行代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ad54d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0306, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#在正向传播函数中执行代码\n",
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear=nn.Linear(20,20)\n",
    "        #因为设置的是False，所以是不参与求导的，不参与训练的\n",
    "        self.rand_weight=torch.rand((20,20),requires_grad=False)\n",
    "        \n",
    "    def forward(self,X):\n",
    "        X=self.linear(X)\n",
    "        X=F.relu(torch.mm(X,self.rand_weight)+1)\n",
    "        X=self.linear(X)\n",
    "        while X.abs().sum() > 1:\n",
    "            X/=2\n",
    "        \n",
    "        return X.sum()\n",
    "    \n",
    "net=FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd34da35",
   "metadata": {},
   "source": [
    "## 混合搭配各种组合块的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e1bf625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.2247, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net=nn.Sequential(nn.Linear(20,64),nn.ReLU(),nn.Linear(64,32),nn.ReLU())\n",
    "        self.linear=nn.Linear(32,16)\n",
    "        \n",
    "    def forward(self,X):\n",
    "        X=self.net(X)\n",
    "        X=self.linear(X)\n",
    "        \n",
    "        return X   \n",
    "net=nn.Sequential(NestMLP(),nn.Linear(16,20),FixedHiddenMLP())\n",
    "net(X)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6c3ffa",
   "metadata": {},
   "source": [
    "# 参数管理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c79d7b",
   "metadata": {},
   "source": [
    "## 首先关注具有单隐藏层的多层感知机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8394cfe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2999],\n",
       "        [0.3167]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "net=nn.Sequential(nn.Linear(4,8),nn.ReLU(),nn.Linear(8,1))\n",
    "X=torch.rand(2,4)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6236589f",
   "metadata": {},
   "source": [
    "## 参数访问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d6fad71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[ 0.2862,  0.3521,  0.0053,  0.2196, -0.0531,  0.2587, -0.2426, -0.1172]])), ('bias', tensor([0.0831]))])\n",
      "OrderedDict([('0.weight', tensor([[-0.3569, -0.4068,  0.3619,  0.3387],\n",
      "        [ 0.2706, -0.1916,  0.1260,  0.2155],\n",
      "        [ 0.1978, -0.4180,  0.4577, -0.3133],\n",
      "        [-0.3524, -0.0809,  0.4494,  0.2720],\n",
      "        [-0.4618, -0.4195,  0.2854,  0.1309],\n",
      "        [-0.0341,  0.2848, -0.3511,  0.0365],\n",
      "        [ 0.0949, -0.3319, -0.1185,  0.3313],\n",
      "        [-0.2899, -0.2630,  0.2026, -0.0393]])), ('0.bias', tensor([ 0.1191,  0.4609,  0.3057, -0.0077,  0.1718, -0.0971,  0.1261, -0.0232])), ('2.weight', tensor([[ 0.2862,  0.3521,  0.0053,  0.2196, -0.0531,  0.2587, -0.2426, -0.1172]])), ('2.bias', tensor([0.0831]))])\n"
     ]
    }
   ],
   "source": [
    "#把每一层里面的那个权重拿出来\n",
    "#nn.Sequential可以简单的理解为python的一个list\n",
    "#这是拿到的最后一层的参数即权重和偏置nn.Linear(8,1)\n",
    "#全连接层的参数就是权重和偏置\n",
    "print(net[2].state_dict())\n",
    "print(net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c80c4cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([0.0831], requires_grad=True)\n",
      "tensor([0.0831])\n",
      "None\n",
      "<class 'torch.nn.parameter.Parameter'>\n"
     ]
    }
   ],
   "source": [
    "#可以访问一些具体的参数\n",
    "#访问偏置的内容\n",
    "print(net[2].bias)\n",
    "#访问偏置的值\n",
    "print(net[2].bias.data)\n",
    "#访问权重的梯度\n",
    "print(net[2].bias.grad)\n",
    "#查看bias的数据类型\n",
    "print(type(net[2].bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fb6b1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "#访问权重的梯度\n",
    "#因为还么有反向训练，所以是None\n",
    "print(net[2].weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a69a909",
   "metadata": {},
   "source": [
    "### 一次性访问所有参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a3cea6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.3569, -0.4068,  0.3619,  0.3387],\n",
      "        [ 0.2706, -0.1916,  0.1260,  0.2155],\n",
      "        [ 0.1978, -0.4180,  0.4577, -0.3133],\n",
      "        [-0.3524, -0.0809,  0.4494,  0.2720],\n",
      "        [-0.4618, -0.4195,  0.2854,  0.1309],\n",
      "        [-0.0341,  0.2848, -0.3511,  0.0365],\n",
      "        [ 0.0949, -0.3319, -0.1185,  0.3313],\n",
      "        [-0.2899, -0.2630,  0.2026, -0.0393]], requires_grad=True)\n",
      "torch.Size([8, 4])\n",
      "0.bias\n",
      "Parameter containing:\n",
      "tensor([ 0.1191,  0.4609,  0.3057, -0.0077,  0.1718, -0.0971,  0.1261, -0.0232],\n",
      "       requires_grad=True)\n",
      "torch.Size([8])\n",
      "2.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.2862,  0.3521,  0.0053,  0.2196, -0.0531,  0.2587, -0.2426, -0.1172]],\n",
      "       requires_grad=True)\n",
      "torch.Size([1, 8])\n",
      "2.bias\n",
      "Parameter containing:\n",
      "tensor([0.0831], requires_grad=True)\n",
      "torch.Size([1])\n",
      "weight\n",
      "Parameter containing:\n",
      "tensor([[-0.3569, -0.4068,  0.3619,  0.3387],\n",
      "        [ 0.2706, -0.1916,  0.1260,  0.2155],\n",
      "        [ 0.1978, -0.4180,  0.4577, -0.3133],\n",
      "        [-0.3524, -0.0809,  0.4494,  0.2720],\n",
      "        [-0.4618, -0.4195,  0.2854,  0.1309],\n",
      "        [-0.0341,  0.2848, -0.3511,  0.0365],\n",
      "        [ 0.0949, -0.3319, -0.1185,  0.3313],\n",
      "        [-0.2899, -0.2630,  0.2026, -0.0393]], requires_grad=True)\n",
      "torch.Size([8, 4])\n",
      "bias\n",
      "Parameter containing:\n",
      "tensor([ 0.1191,  0.4609,  0.3057, -0.0077,  0.1718, -0.0971,  0.1261, -0.0232],\n",
      "       requires_grad=True)\n",
      "torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "#一次性访问所有参数\n",
    "#relu没有参数，所以拿不出来\n",
    "#named_parameters()是访问所有的参数\n",
    "for name,param in net.named_parameters():\n",
    "    print(name)\n",
    "    print(param)\n",
    "    print(param.shape)\n",
    "\n",
    "#访问第0层的所有参数\n",
    "for name,param in net[0].named_parameters():\n",
    "    print(name)\n",
    "    print(param)\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeb42f6",
   "metadata": {},
   "source": [
    "### 根据名字访问指定的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27dff6dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0831])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#由上一步输出所有参数的名字，我们可以根据名字来获取指定的参数\n",
    "net.state_dict()['2.bias'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d489863b",
   "metadata": {},
   "source": [
    "### 从嵌套块收集参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71a6c2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0598],\n",
      "        [-0.0598]], grad_fn=<AddmmBackward>)\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 4),\n",
    "                         nn.ReLU())\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        #这里add_module的好处是说，可以写字符进去f'block {i}'，可以自己命名块的名字\n",
    "        net.add_module(f'block {i}', block1())\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(block2(), nn.Linear(4, 1))\n",
    "print(rgnet(X))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709a86a3",
   "metadata": {},
   "source": [
    "## 通过print（net)大致了解这个网络内部什么样子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08c6958b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block 0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#可以通过print（net)大致了解这个网络内部什么样子\n",
    "#三个Sequential\n",
    "print(rgnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4cd46b",
   "metadata": {},
   "source": [
    "## 内置初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9c1a9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0130, -0.0017, -0.0074,  0.0083]) tensor(0.)\n",
      "tensor([[ 0.0130, -0.0017, -0.0074,  0.0083],\n",
      "        [ 0.0058,  0.0021, -0.0001,  0.0039],\n",
      "        [-0.0080,  0.0180, -0.0065, -0.0146],\n",
      "        [-0.0022, -0.0098, -0.0011, -0.0021],\n",
      "        [-0.0010, -0.0012,  0.0143,  0.0098],\n",
      "        [ 0.0073,  0.0086,  0.0115, -0.0022],\n",
      "        [-0.0051,  0.0058,  0.0104,  0.0048],\n",
      "        [ 0.0100,  0.0063,  0.0217, -0.0034]]) tensor([0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([ 0.0027,  0.0119, -0.0098,  0.0123]) tensor(0.)\n",
      "torch.Size([8, 4]) torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "#如何去修改默认的初始函数，这里主要是修改了初始的权重和偏置\n",
    "#m is Module\n",
    "#nn.init里面写了很多初始化的函数\n",
    "def init_normal(m):\n",
    "    if type(m)==nn.Linear:\n",
    "        #下划线是指不返回一个值，而是用这个值直接去改写权重，满足一个均值为0，方差为0.01的正态分布\n",
    "        nn.init.normal_(m.weight,mean=0,std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "#这个apply相当于一个循环，把init_normal初始化函数应用到net里面每一个layer\n",
    "net.apply(init_normal)\n",
    "print(net[0].weight.data[0],net[0].bias.data[0])\n",
    "\n",
    "#如果data不加0的话，那第0层有很多权重和偏置，就都会输出来，加0只是输出第一个\n",
    "print(net[0].weight.data,net[0].bias.data)\n",
    "\n",
    "def init_constant(m):\n",
    "    if type(m)==nn.Linear:\n",
    "        #下划线是指不返回一个值，而是用这个值直接去改写权重，满足一个均值为0，方差为0.01的正态分布\n",
    "        #constant是指一个固定值\n",
    "        nn.init.constant_(m.weight,1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "        \n",
    "net.apply(init_normal)\n",
    "print(net[0].weight.data[0],net[0].bias.data[0]) \n",
    "print(net[0].weight.shape,net[0].bias.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c92beb5",
   "metadata": {},
   "source": [
    "### 对某些块应用不同的初始化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6bc0131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0754,  0.3403, -0.6856, -0.0308])\n",
      "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "#用的是vavier的uniform distribution(均匀分布)初始化\n",
    "def xavier(m):\n",
    "    if type(m)==nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "#固定值42\n",
    "def init_42(m):\n",
    "    if type(m)==nn.Linear:\n",
    "        nn.init.constant_(m.weight,42)\n",
    "        \n",
    "#net可以调用apply,那么net的每一层也都可以单独调用        \n",
    "net[0].apply(xavier)\n",
    "net[2].apply(init_42)\n",
    "print(net[0].weight.data[0])\n",
    "print(net[2].weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7441127b",
   "metadata": {},
   "source": [
    "## 自定义初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57828258",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch14] *",
   "language": "python",
   "name": "conda-env-torch14-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
