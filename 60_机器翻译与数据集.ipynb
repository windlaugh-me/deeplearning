{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e996176",
   "metadata": {},
   "source": [
    "# 机器翻译与数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ddc8df1",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 1455] 页面文件太小，无法完成操作。 Error loading \"E:\\anaconda\\envs\\torch14\\lib\\site-packages\\torch\\lib\\caffe2_detectron_ops_gpu.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01md2l\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m torch \u001b[38;5;28;01mas\u001b[39;00m d2l\n",
      "File \u001b[1;32mE:\\anaconda\\envs\\torch14\\lib\\site-packages\\torch\\__init__.py:123\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    121\u001b[0m     err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(last_error)\n\u001b[0;32m    122\u001b[0m     err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    125\u001b[0m     is_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 1455] 页面文件太小，无法完成操作。 Error loading \"E:\\anaconda\\envs\\torch14\\lib\\site-packages\\torch\\lib\\caffe2_detectron_ops_gpu.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c0c429",
   "metadata": {},
   "source": [
    "## 下载和预处理数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb85cbc0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd2l' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#法语到英语的翻译\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m d2l\u001b[38;5;241m.\u001b[39mDATA_HUB[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfra-eng\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[43md2l\u001b[49m\u001b[38;5;241m.\u001b[39mDATA_URL \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfra-eng.zip\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      3\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m94646ad1522d915e7b0f9296181140edcf86a4f5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_data_nmt\u001b[39m():\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124;03m\"\"\"载入“英语－法语”数据集。\"\"\"\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'd2l' is not defined"
     ]
    }
   ],
   "source": [
    "#法语到英语的翻译\n",
    "#数据集里面本来就是一个个词\n",
    "d2l.DATA_HUB['fra-eng'] = (d2l.DATA_URL + 'fra-eng.zip',\n",
    "                           '94646ad1522d915e7b0f9296181140edcf86a4f5')\n",
    "\n",
    "def read_data_nmt():\n",
    "    \"\"\"载入“英语－法语”数据集。\"\"\"\n",
    "    data_dir = d2l.download_extract('fra-eng')\n",
    "    with open(os.path.join(data_dir, 'fra.txt'), 'r') as f:\n",
    "        return f.read()\n",
    "\n",
    "raw_text = read_data_nmt()\n",
    "print(raw_text[:75])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1538f6",
   "metadata": {},
   "source": [
    "## 几个预处理步骤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ed4645a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'raw_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     out \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m char \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m no_space(char, text[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01melse\u001b[39;00m char\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, char \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(text)]\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(out)\n\u001b[1;32m---> 14\u001b[0m text \u001b[38;5;241m=\u001b[39m preprocess_nmt(\u001b[43mraw_text\u001b[49m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(text[:\u001b[38;5;241m80\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'raw_text' is not defined"
     ]
    }
   ],
   "source": [
    "#把标点符号留下来 把标点符号\n",
    "def preprocess_nmt(text):\n",
    "    \"\"\"预处理“英语－法语”数据集。\"\"\"\n",
    "    #如果标点前面没有空格的话，把标点前面加一个空格，这样就可以当作一个词切出来\n",
    "    def no_space(char, prev_char):\n",
    "        return char in set(',.!?') and prev_char != ' '\n",
    "\n",
    "    text = text.replace('\\u202f', ' ').replace('\\xa0', ' ').lower()\n",
    "    out = [\n",
    "        ' ' + char if i > 0 and no_space(char, text[i - 1]) else char\n",
    "        for i, char in enumerate(text)]\n",
    "    return ''.join(out)\n",
    "\n",
    "text = preprocess_nmt(raw_text)\n",
    "print(text[:80])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6208677",
   "metadata": {},
   "source": [
    "## 词元化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fb30318",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m             target\u001b[38;5;241m.\u001b[39mappend(parts[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m source, target\n\u001b[1;32m---> 14\u001b[0m source, target \u001b[38;5;241m=\u001b[39m tokenize_nmt(\u001b[43mtext\u001b[49m)\n\u001b[0;32m     15\u001b[0m source[:\u001b[38;5;241m6\u001b[39m], target[:\u001b[38;5;241m6\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "#这里没有用字符，用的是词\n",
    "def tokenize_nmt(text, num_examples=None):\n",
    "    \"\"\"词元化“英语－法语”数据数据集。\"\"\"\n",
    "    source, target = [], []\n",
    "    #text.split('\\n')是把读到的内容根据’换行‘切分\n",
    "    for i, line in enumerate(text.split('\\n')):\n",
    "        if num_examples and i > num_examples:\n",
    "            break\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) == 2:\n",
    "            source.append(parts[0].split(' '))\n",
    "            target.append(parts[1].split(' '))\n",
    "    return source, target\n",
    "\n",
    "source, target = tokenize_nmt(text)\n",
    "source[:6], target[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec0493e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你是 我的老师\n",
      "['你是', '我的老师']\n"
     ]
    }
   ],
   "source": [
    "text_01 = '你是 我的老师'\n",
    "print(text_01)\n",
    "list_text_01 = text_01.split(' ')\n",
    "print(list_text_01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd125dde",
   "metadata": {},
   "source": [
    "## 绘制每个文本序列所包含的标记数量的直方图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7d41429",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd2l' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43md2l\u001b[49m\u001b[38;5;241m.\u001b[39mset_figsize()\n\u001b[0;32m      2\u001b[0m _, _, patches \u001b[38;5;241m=\u001b[39m d2l\u001b[38;5;241m.\u001b[39mplt\u001b[38;5;241m.\u001b[39mhist([[\u001b[38;5;28mlen\u001b[39m(l)\n\u001b[0;32m      3\u001b[0m                                \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m source], [\u001b[38;5;28mlen\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m target]],\n\u001b[0;32m      4\u001b[0m                              label\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m patch \u001b[38;5;129;01min\u001b[39;00m patches[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpatches:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'd2l' is not defined"
     ]
    }
   ],
   "source": [
    "d2l.set_figsize()\n",
    "_, _, patches = d2l.plt.hist([[len(l)\n",
    "                               for l in source], [len(l) for l in target]],\n",
    "                             label=['source', 'target'])\n",
    "for patch in patches[1].patches:\n",
    "    patch.set_hatch('/')\n",
    "d2l.plt.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec14721",
   "metadata": {},
   "source": [
    "## 词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed9877e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd2l' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#'<pad>', '<bos>', '<eos>']分别是填充，开头，结尾\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m src_vocab \u001b[38;5;241m=\u001b[39m \u001b[43md2l\u001b[49m\u001b[38;5;241m.\u001b[39mVocab(source, min_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m      3\u001b[0m                       reserved_tokens\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<bos>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<eos>\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mlen\u001b[39m(src_vocab)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'd2l' is not defined"
     ]
    }
   ],
   "source": [
    "#'<pad>', '<bos>', '<eos>']分别是填充，开头，结尾\n",
    "src_vocab = d2l.Vocab(source, min_freq=2,\n",
    "                      reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "len(src_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29625de0",
   "metadata": {},
   "source": [
    "## 序列样本都有一个固定的长度 截断或填充文本序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4f8554c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'src_vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m line[:num_steps]\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m line \u001b[38;5;241m+\u001b[39m [padding_token] \u001b[38;5;241m*\u001b[39m (num_steps \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(line))\n\u001b[1;32m----> 8\u001b[0m truncate_pad(\u001b[43msrc_vocab\u001b[49m[source[\u001b[38;5;241m0\u001b[39m]], \u001b[38;5;241m10\u001b[39m, src_vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'src_vocab' is not defined"
     ]
    }
   ],
   "source": [
    "#句子不能像语言模型那样切切\n",
    "def truncate_pad(line, num_steps, padding_token):\n",
    "    \"\"\"截断或填充文本序列。\"\"\"\n",
    "    if len(line) > num_steps:\n",
    "        return line[:num_steps]\n",
    "    return line + [padding_token] * (num_steps - len(line))\n",
    "\n",
    "truncate_pad(src_vocab[source[0]], 10, src_vocab['<pad>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89822a14",
   "metadata": {},
   "source": [
    "## 转换成小批量数据集用于训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23c03174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_array_nmt(lines, vocab, num_steps):\n",
    "    \"\"\"将机器翻译的文本序列转换成小批量。\"\"\"\n",
    "    lines = [vocab[l] for l in lines]\n",
    "    #加一个截止符，告诉模型什么时候这个句子结束了\n",
    "    lines = [l + [vocab['<eos>']] for l in lines]\n",
    "    array = torch.tensor([\n",
    "        truncate_pad(l, num_steps, vocab['<pad>']) for l in lines])\n",
    "    valid_len = (array != vocab['<pad>']).type(torch.int32).sum(1)\n",
    "    #很多填充的东西都是没有意义的，valid_len是原始的有意义的\n",
    "    return array, valid_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a4ca8a",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46734f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_steps是每个句子的长度\n",
    "def load_data_nmt(batch_size, num_steps, num_examples=600):\n",
    "    \"\"\"返回翻译数据集的迭代器和词汇表。\"\"\"\n",
    "    text = preprocess_nmt(read_data_nmt())\n",
    "    source, target = tokenize_nmt(text, num_examples)\n",
    "    src_vocab = d2l.Vocab(source, min_freq=2,\n",
    "                          reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "    tgt_vocab = d2l.Vocab(target, min_freq=2,\n",
    "                          reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "    src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)\n",
    "    tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)\n",
    "    data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)\n",
    "    data_iter = d2l.load_array(data_arrays, batch_size)\n",
    "    return data_iter, src_vocab, tgt_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a778c91f",
   "metadata": {},
   "source": [
    "## 读出“英语－法语”数据集中的第一个小批量数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87d5dbcd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess_nmt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_iter, src_vocab, tgt_vocab \u001b[38;5;241m=\u001b[39m \u001b[43mload_data_nmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X, X_valid_len, Y, Y_valid_len \u001b[38;5;129;01min\u001b[39;00m train_iter:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX:\u001b[39m\u001b[38;5;124m'\u001b[39m, X\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mint32))\n",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36mload_data_nmt\u001b[1;34m(batch_size, num_steps, num_examples)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data_nmt\u001b[39m(batch_size, num_steps, num_examples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m600\u001b[39m):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124;03m\"\"\"返回翻译数据集的迭代器和词汇表。\"\"\"\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_nmt\u001b[49m(read_data_nmt())\n\u001b[0;32m      5\u001b[0m     source, target \u001b[38;5;241m=\u001b[39m tokenize_nmt(text, num_examples)\n\u001b[0;32m      6\u001b[0m     src_vocab \u001b[38;5;241m=\u001b[39m d2l\u001b[38;5;241m.\u001b[39mVocab(source, min_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m      7\u001b[0m                           reserved_tokens\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<bos>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<eos>\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocess_nmt' is not defined"
     ]
    }
   ],
   "source": [
    "train_iter, src_vocab, tgt_vocab = load_data_nmt(batch_size=2, num_steps=8)\n",
    "for X, X_valid_len, Y, Y_valid_len in train_iter:\n",
    "    print('X:', X.type(torch.int32))\n",
    "    print('valid lengths for X:', X_valid_len)\n",
    "    print('Y:', Y.type(torch.int32))\n",
    "    print('valid lengths for Y:', Y_valid_len)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d0b7be",
   "metadata": {},
   "source": [
    "## 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb4bc68",
   "metadata": {},
   "source": [
    "- 生成的Y不再是一个标签，也是一个向量\n",
    "- 填充 valid_len记录原始的长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f0d6d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch14] *",
   "language": "python",
   "name": "conda-env-torch14-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
