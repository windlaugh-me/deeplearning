{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ca243e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May 19 22:00:05 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 511.79       Driver Version: 511.79       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   68C    P0   103W /  N/A |   5199MiB /  8192MiB |     66%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      6680      C   ...a\\envs\\torch14\\python.exe    N/A      |\n",
      "|    0   N/A  N/A     11720    C+G   ...mmandCenterBackground.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31522d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#深度学习框架都是默认在CPU上做运算，需要去指定在GPU上做运算\n",
    "import torch\n",
    "from torch import nn\n",
    "#cpu\n",
    "torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e99c554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.cuda.device at 0x23188e0aeb0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GPU,默认是0号GPU\n",
    "torch.cuda.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2f3fbd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.cuda.device at 0x23188e2dee0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#查询第1号gpu\n",
    "torch.cuda.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bcd661a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#查询可用GPU的数量\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "044f3029",
   "metadata": {},
   "outputs": [],
   "source": [
    "#这两个函数允许我们在请求的GPU不存在的情况下运行代码\n",
    "def try_gpu(i=0):\n",
    "    if torch.cuda.device_count()>=i+1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e46fa5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cuda', index=0),\n",
       " device(type='cpu'),\n",
       " [device(type='cuda', index=0)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#返回所有可用的GPU\n",
    "def try_all_gpus():  \n",
    "    \"\"\"返回所有可用的GPU，如果没有GPU，则返回[cpu(),]。\"\"\"\n",
    "    devices = [\n",
    "        torch.device(f'cuda:{i}') for i in range(torch.cuda.device_count())]\n",
    "    return devices if devices else [torch.device('cpu')]\n",
    "\n",
    "try_gpu(), try_gpu(10), try_all_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2e35d0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#查询张量所在的设备\n",
    "#默认是在cpu的内存上\n",
    "x=torch.tensor([0,1,2,3])\n",
    "x.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d635a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#创建的时候可以放在GPU上\n",
    "x=torch.ones(2,3,device=try_gpu())\n",
    "x.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8a89ccf",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: invalid device ordinal",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#要计算x+y,我们需要决定在哪里执行这个操作\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#默认是cpu上，如果我们要计算，我们需要确定X和Y在同一个设备上，同一个CPU或者同一个GPU\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#这样就把x放到了1号GPU\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#如果不这样想的话，可能创建网络的时候所有层没有在同一个GPU之上，那就需要来回挪数据，那网络的性能就会受到影响\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m z\u001b[38;5;241m=\u001b[39m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: invalid device ordinal"
     ]
    }
   ],
   "source": [
    "#要计算x+y,我们需要决定在哪里执行这个操作\n",
    "#默认是cpu上，如果我们要计算，我们需要确定X和Y在同一个设备上，同一个CPU或者同一个GPU\n",
    "#这样就把x放到了1号GPU\n",
    "#如果不这样想的话，可能创建网络的时候所有层没有在同一个GPU之上，那就需要来回挪数据，那网络的性能就会受到影响\n",
    "z=x.cuda(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06d45b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0249],\n",
       "        [-0.0249]], device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#神经网络与GPU\n",
    "#通常在cpu上把权重初始化好\n",
    "net=nn.Sequential(nn.Linear(3,1))\n",
    "#手动copy一份放到GPU\n",
    "net=net.to(device=try_gpu())\n",
    "\n",
    "#0号GPU做运算\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c35525db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#确认模型参数存储在同一个GPU\n",
    "net[0].weight.data.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f397f0f3",
   "metadata": {},
   "source": [
    "# 答疑"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3258c64b",
   "metadata": {},
   "source": [
    "+ 需要手动copy把初始权重，输入，模型都放到GPU。这样就可以做forward,然后backward也是在GPU上\n",
    "+ 一般data在network之前to gpu,因为节省GPU去训练网络，第二个是gpu不一定能很好的处理那些数据变化。如果数据读的比GPU计算快的，尽量在cpu上进行数据处理。\n",
    "+ GPU上推理是指的forward\n",
    "+ 默认是不共享参数的，每次实例都是新的参数。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch14] *",
   "language": "python",
   "name": "conda-env-torch14-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
