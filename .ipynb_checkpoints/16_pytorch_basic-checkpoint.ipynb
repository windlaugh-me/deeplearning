{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66f0360e",
   "metadata": {},
   "source": [
    "# 模型构造"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1b297c",
   "metadata": {},
   "source": [
    "## 层和块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1091b82",
   "metadata": {},
   "source": [
    "## 回顾一下多层感知机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bac63241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1894, -0.0263,  0.1651, -0.0563, -0.1680, -0.2232, -0.0469, -0.0507,\n",
       "          0.1376,  0.2211],\n",
       "        [-0.1639,  0.0142,  0.1997,  0.0057, -0.0349, -0.1787, -0.0469,  0.0455,\n",
       "          0.1480,  0.1851]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#回顾一下多层感知机\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "#nn.Sequential定义了一个特殊的Module\n",
    "net=nn.Sequential(nn.Linear(20,256),nn.ReLU(),nn.Linear(256,10))\n",
    "\n",
    "#随机生成一个2*20的矩阵，满足均匀分布\n",
    "X=torch.rand(2,20)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dd6470",
   "metadata": {},
   "source": [
    "## 自定义MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10fa4db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1470, -0.1755,  0.1342, -0.2123, -0.0479, -0.1716,  0.0633, -0.2490,\n",
       "         -0.0471,  0.1435],\n",
       "        [ 0.0724, -0.0842,  0.1266, -0.1585, -0.0415, -0.2796,  0.0993, -0.1747,\n",
       "         -0.0793,  0.1534]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#任何一个层和神经网络都是Module的子类\n",
    "#自定义一个MLP\n",
    "#MLP就可以继承Module中很多的函数\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    #这个函数就用来定义一些参数和类\n",
    "    #网络中调用的所有层都在这个函数里面\n",
    "    def __init__(self):\n",
    "        #调用父类，把一些内部参数全部设置好，这样在初始化weight可以全部弄好\n",
    "        super().__init__()\n",
    "        self.hidden=nn.Linear(20,256)\n",
    "        self.out=nn.Linear(256,10)\n",
    "    #前向计算\n",
    "    def forward(self,X):\n",
    "        return self.out(F.relu(self.hidden(X)))\n",
    "\n",
    "#实例化多层感知机的层，每次调用正向传播函数时调用这些层\n",
    "net=MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fb5d89",
   "metadata": {},
   "source": [
    "## 实现nn.Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79d7701",
   "metadata": {},
   "source": [
    "#实现nn.Sequential\n",
    "class MySequential(nn.Module):\n",
    "    def __init__(self,*args):#args里面存放各个层\n",
    "        super().__init__()\n",
    "        for block in args:\n",
    "            #做成一个字典\n",
    "            self._modules[block]=block\n",
    "            \n",
    "    def forward(self,X):\n",
    "        #把各个层取出来，输入X得到一个输出，不断按顺序经过每一层\n",
    "        for block in self._modules.values():\n",
    "            X=block(X)\n",
    "            #print(self._modules)\n",
    "            #print(self._modules.values())\n",
    "        return X\n",
    "    \n",
    "net=MySequential(nn.Linear(20,256),nn.ReLU(),nn.Linear(256,10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5271c389",
   "metadata": {},
   "source": [
    "## 在正向传播函数中执行代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ad54d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0970, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#在正向传播函数中执行代码\n",
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear=nn.Linear(20,20)\n",
    "        #因为设置的是False，所以是不参与求导的，不参与训练的\n",
    "        self.rand_weight=torch.rand((20,20),requires_grad=False)\n",
    "        \n",
    "    def forward(self,X):\n",
    "        X=self.linear(X)\n",
    "        X=F.relu(torch.mm(X,self.rand_weight)+1)\n",
    "        X=self.linear(X)\n",
    "        while X.abs().sum() > 1:\n",
    "            X/=2\n",
    "        \n",
    "        return X.sum()\n",
    "    \n",
    "net=FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd34da35",
   "metadata": {},
   "source": [
    "## 混合搭配各种组合块的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e1bf625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3440, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net=nn.Sequential(nn.Linear(20,64),nn.ReLU(),nn.Linear(64,32),nn.ReLU())\n",
    "        self.linear=nn.Linear(32,16)\n",
    "        \n",
    "    def forward(self,X):\n",
    "        X=self.net(X)\n",
    "        X=self.linear(X)\n",
    "        \n",
    "        return X   \n",
    "net=nn.Sequential(NestMLP(),nn.Linear(16,20),FixedHiddenMLP())\n",
    "net(X)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6c3ffa",
   "metadata": {},
   "source": [
    "# 参数管理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c79d7b",
   "metadata": {},
   "source": [
    "## 首先关注具有单隐藏层的多层感知机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8394cfe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1797],\n",
       "        [0.1988]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "net=nn.Sequential(nn.Linear(4,8),nn.ReLU(),nn.Linear(8,1))\n",
    "X=torch.rand(2,4)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6236589f",
   "metadata": {},
   "source": [
    "## 参数访问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d6fad71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[ 0.2941,  0.2297,  0.2031, -0.3107,  0.1994, -0.1032,  0.2746, -0.0718]])), ('bias', tensor([0.2866]))])\n",
      "OrderedDict([('0.weight', tensor([[ 0.1941, -0.2631,  0.4697, -0.1283],\n",
      "        [ 0.3978, -0.0353, -0.1617, -0.4579],\n",
      "        [ 0.3626, -0.3508,  0.0534, -0.3789],\n",
      "        [ 0.4527,  0.4131, -0.2868, -0.4685],\n",
      "        [-0.4962, -0.4026,  0.4060, -0.1589],\n",
      "        [ 0.0138,  0.0305, -0.4418,  0.0507],\n",
      "        [ 0.0321,  0.4760, -0.2706, -0.1939],\n",
      "        [ 0.1384,  0.4468,  0.1653, -0.4860]])), ('0.bias', tensor([ 0.0052,  0.4538, -0.3367,  0.2529,  0.1838,  0.1869, -0.2166,  0.1582])), ('2.weight', tensor([[ 0.2941,  0.2297,  0.2031, -0.3107,  0.1994, -0.1032,  0.2746, -0.0718]])), ('2.bias', tensor([0.2866]))])\n"
     ]
    }
   ],
   "source": [
    "#把每一层里面的那个权重拿出来\n",
    "#nn.Sequential可以简单的理解为python的一个list\n",
    "#这是拿到的最后一层的参数即权重和偏置nn.Linear(8,1)\n",
    "#全连接层的参数就是权重和偏置\n",
    "print(net[2].state_dict())\n",
    "print(net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c80c4cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([0.2866], requires_grad=True)\n",
      "tensor([0.2866])\n",
      "None\n",
      "<class 'torch.nn.parameter.Parameter'>\n"
     ]
    }
   ],
   "source": [
    "#可以访问一些具体的参数\n",
    "#访问偏置的内容\n",
    "print(net[2].bias)\n",
    "#访问偏置的值\n",
    "print(net[2].bias.data)\n",
    "#访问权重的梯度\n",
    "print(net[2].bias.grad)\n",
    "#查看bias的数据类型\n",
    "print(type(net[2].bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fb6b1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "#访问权重的梯度\n",
    "#因为还么有反向训练，所以是None\n",
    "print(net[2].weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a69a909",
   "metadata": {},
   "source": [
    "### 一次性访问所有参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a3cea6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight\n",
      "Parameter containing:\n",
      "tensor([[-5.9244, -6.1343,  6.1429,  0.0000],\n",
      "        [ 0.0000,  0.0000, -7.9209,  9.7815],\n",
      "        [-9.4067,  9.5231,  9.2864, -0.0000],\n",
      "        [-6.7020, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000, -9.8263,  0.0000],\n",
      "        [-8.3893,  0.0000, -9.2670, -5.3232],\n",
      "        [-7.6139, -5.4609,  7.5534, -0.0000],\n",
      "        [ 9.2938, -0.0000, -9.1196, -0.0000]], requires_grad=True)\n",
      "torch.Size([8, 4])\n",
      "0.bias\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "torch.Size([8])\n",
      "2.weight\n",
      "Parameter containing:\n",
      "tensor([[0.0000, 0.0000, -0.0000, 0.0000, -0.0000, -0.0000, -0.0000, 5.8519]],\n",
      "       requires_grad=True)\n",
      "torch.Size([1, 8])\n",
      "2.bias\n",
      "Parameter containing:\n",
      "tensor([0.], requires_grad=True)\n",
      "torch.Size([1])\n",
      "weight\n",
      "Parameter containing:\n",
      "tensor([[-5.9244, -6.1343,  6.1429,  0.0000],\n",
      "        [ 0.0000,  0.0000, -7.9209,  9.7815],\n",
      "        [-9.4067,  9.5231,  9.2864, -0.0000],\n",
      "        [-6.7020, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000, -9.8263,  0.0000],\n",
      "        [-8.3893,  0.0000, -9.2670, -5.3232],\n",
      "        [-7.6139, -5.4609,  7.5534, -0.0000],\n",
      "        [ 9.2938, -0.0000, -9.1196, -0.0000]], requires_grad=True)\n",
      "torch.Size([8, 4])\n",
      "bias\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "#一次性访问所有参数\n",
    "#relu没有参数，所以拿不出来\n",
    "#named_parameters()是访问所有的参数\n",
    "for name,param in net.named_parameters():\n",
    "    print(name)\n",
    "    print(param)\n",
    "    print(param.shape)\n",
    "\n",
    "#访问第0层的所有参数\n",
    "for name,param in net[0].named_parameters():\n",
    "    print(name)\n",
    "    print(param)\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeb42f6",
   "metadata": {},
   "source": [
    "### 根据名字访问指定的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27dff6dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2866])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#由上一步输出所有参数的名字，我们可以根据名字来获取指定的参数\n",
    "net.state_dict()['2.bias'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d489863b",
   "metadata": {},
   "source": [
    "### 从嵌套块收集参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71a6c2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4549],\n",
      "        [0.4549]], grad_fn=<AddmmBackward>)\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 4),\n",
    "                         nn.ReLU())\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        #这里add_module的好处是说，可以写字符进去f'block {i}'，可以自己命名块的名字\n",
    "        net.add_module(f'block {i}', block1())\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(block2(), nn.Linear(4, 1))\n",
    "print(rgnet(X))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709a86a3",
   "metadata": {},
   "source": [
    "## 通过print（net)大致了解这个网络内部什么样子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08c6958b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block 0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#可以通过print（net)大致了解这个网络内部什么样子\n",
    "#三个Sequential\n",
    "print(rgnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4cd46b",
   "metadata": {},
   "source": [
    "## 内置初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9c1a9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0053, -0.0048, -0.0011,  0.0006]) tensor(0.)\n",
      "tensor([[ 0.0053, -0.0048, -0.0011,  0.0006],\n",
      "        [-0.0023,  0.0040, -0.0052, -0.0011],\n",
      "        [-0.0051,  0.0030, -0.0173, -0.0058],\n",
      "        [-0.0103,  0.0102, -0.0024,  0.0027],\n",
      "        [-0.0079, -0.0044, -0.0049,  0.0022],\n",
      "        [-0.0067,  0.0151,  0.0070,  0.0021],\n",
      "        [-0.0147,  0.0077, -0.0129, -0.0159],\n",
      "        [-0.0068,  0.0009, -0.0069, -0.0021]]) tensor([0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.0147, 0.0057, 0.0254, 0.0312]) tensor(0.)\n",
      "torch.Size([8, 4]) torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "#如何去修改默认的初始函数，这里主要是修改了初始的权重和偏置\n",
    "#m is Module\n",
    "#nn.init里面写了很多初始化的函数\n",
    "def init_normal(m):\n",
    "    if type(m)==nn.Linear:\n",
    "        #下划线是指不返回一个值，而是用这个值直接去改写权重，满足一个均值为0，方差为0.01的正态分布\n",
    "        nn.init.normal_(m.weight,mean=0,std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "#这个apply相当于一个循环，把init_normal初始化函数应用到net里面每一个layer\n",
    "net.apply(init_normal)\n",
    "print(net[0].weight.data[0],net[0].bias.data[0])\n",
    "\n",
    "#如果data不加0的话，那第0层有很多权重和偏置，就都会输出来，加0只是输出第一个\n",
    "print(net[0].weight.data,net[0].bias.data)\n",
    "\n",
    "def init_constant(m):\n",
    "    if type(m)==nn.Linear:\n",
    "        #下划线是指不返回一个值，而是用这个值直接去改写权重，满足一个均值为0，方差为0.01的正态分布\n",
    "        #constant是指一个固定值\n",
    "        nn.init.constant_(m.weight,1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "        \n",
    "net.apply(init_normal)\n",
    "print(net[0].weight.data[0],net[0].bias.data[0]) \n",
    "print(net[0].weight.shape,net[0].bias.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c92beb5",
   "metadata": {},
   "source": [
    "### 对某些块应用不同的初始化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6bc0131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6618,  0.0305, -0.3702, -0.2544])\n",
      "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "#用的是vavier的uniform distribution(均匀分布)初始化\n",
    "def xavier(m):\n",
    "    if type(m)==nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "#固定值42\n",
    "def init_42(m):\n",
    "    if type(m)==nn.Linear:\n",
    "        nn.init.constant_(m.weight,42)\n",
    "        \n",
    "#net可以调用apply,那么net的每一层也都可以单独调用        \n",
    "net[0].apply(xavier)\n",
    "net[2].apply(init_42)\n",
    "print(net[0].weight.data[0])\n",
    "print(net[2].weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7441127b",
   "metadata": {},
   "source": [
    "## 自定义初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57828258",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight\n",
      "Parameter containing:\n",
      "tensor([[-5.9244, -6.1343,  6.1429,  0.0000],\n",
      "        [ 0.0000,  0.0000, -7.9209,  9.7815],\n",
      "        [-9.4067,  9.5231,  9.2864, -0.0000],\n",
      "        [-6.7020, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000, -9.8263,  0.0000],\n",
      "        [-8.3893,  0.0000, -9.2670, -5.3232],\n",
      "        [-7.6139, -5.4609,  7.5534, -0.0000],\n",
      "        [ 9.2938, -0.0000, -9.1196, -0.0000]], requires_grad=True)\n",
      "torch.Size([8, 4])\n",
      "bias\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "torch.Size([8])\n",
      "weight\n",
      "Parameter containing:\n",
      "tensor([[0.0000, 0.0000, -0.0000, 0.0000, -0.0000, -0.0000, -0.0000, 5.8519]],\n",
      "       requires_grad=True)\n",
      "torch.Size([1, 8])\n",
      "bias\n",
      "Parameter containing:\n",
      "tensor([0.], requires_grad=True)\n",
      "torch.Size([1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.6825, -6.3561, -0.0000,  0.0000],\n",
       "        [-0.0000,  7.3559, -0.0000, -0.0000]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        for name,params in m.named_parameters():\n",
    "            print(name)\n",
    "            print(params)\n",
    "            print(params.shape)\n",
    "        #print( \"Init\",*[(name, param.shape) for name, param in m.named_parameters()][0])\n",
    "        nn.init.uniform_(m.weight, -10, 10)\n",
    "        m.weight.data *= m.weight.data.abs() >= 5\n",
    "\n",
    "net.apply(my_init)\n",
    "net[0].weight[:2]\n",
    "#grad_fn=<SliceBackward>这个参数指明是如何进行指导求导的。程序反向切片，从给出的某一个目标点，获取所有到达该目标点的路径。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2567e489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42.0000, -5.3561,  1.0000,  1.0000])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#直接给某些参数赋值\n",
    "net[0].weight.data[:] += 1\n",
    "net[0].weight.data[0, 0] = 42\n",
    "net[0].weight.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc758cf5",
   "metadata": {},
   "source": [
    "## 参数绑定（不同层之间权重共享）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7cfa272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "#不同层之间权重共享\n",
    "shared = nn.Linear(8, 8)\n",
    "#不管网络如何变化，第2层和第4层的网络参数是一样的\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), shared, nn.ReLU(), shared,nn.ReLU(), nn.Linear(8, 1))\n",
    "net(X)\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "net[2].weight.data[0, 0] = 100\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d416b844",
   "metadata": {},
   "source": [
    "# 自定义层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "66fd9065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2., -1.,  0.,  1.,  2.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#构造一个没有任何参数的自定义层\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "class CenteredLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X - X.mean()\n",
    "\n",
    "#定义一个类的实例\n",
    "layer=CenteredLayer()\n",
    "layer(torch.FloatTensor([1,2,3,4,5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412cea7e",
   "metadata": {},
   "source": [
    "## 将层作为组件合并到构建更复杂的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "744e03bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.6566e-09, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net=nn.Sequential(nn.Linear(8,128),CenteredLayer())\n",
    "\n",
    "X=torch.rand(4,8)\n",
    "net(X).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875949b0",
   "metadata": {},
   "source": [
    "## 带参数的自定义线性层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f6054e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0597,  0.8627,  2.0964],\n",
       "        [ 0.7405, -2.7173,  1.0936],\n",
       "        [ 0.6236,  0.1603,  0.6659],\n",
       "        [ 0.5242, -1.1551,  0.1931],\n",
       "        [ 0.2098,  0.2911, -0.5977]], requires_grad=True)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#我们知道线性层有两个参数，一个是输入维度，一个是输出维度\n",
    "#要定义自带参数的话，要放到nn.Parameter中\n",
    "class MyLinear(nn.Module):\n",
    "    def __init__(self,in_units,units):\n",
    "        super().__init__()\n",
    "        #我们知道之前weight和bias都在parameter里面。随机生成一个矩阵放到这里面，那weight的名称和梯度就都有了\n",
    "        self.weight=nn.Parameter(torch.randn(in_units,units))\n",
    "        self.bias=nn.Parameter(torch.randn(units))\n",
    "        \n",
    "    def forward(self,X):\n",
    "        linear=torch.matmul(X,self.weight.data)+ self.bias.data\n",
    "        return F.relu(linear)\n",
    "    \n",
    "dense=MyLinear(5,3)\n",
    "dense.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a3e160",
   "metadata": {},
   "source": [
    "## 使用自定义曾直接执行正向传播计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e8637122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5242, 0.0000, 2.5220],\n",
       "        [0.9152, 0.0000, 3.7823]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense(torch.rand(2,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2a0ba1",
   "metadata": {},
   "source": [
    "## 使用自定义层构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "51f3c8a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net=nn.Sequential(nn.Linear(64,8),MyLinear(8,1))\n",
    "net(torch.rand(2,64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496f2751",
   "metadata": {},
   "source": [
    "# 读写文件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c794f978",
   "metadata": {},
   "source": [
    "## 加载和保存张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fec9477e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#加载和保存张量\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "x=torch.arange(4)\n",
    "torch.save(x,'x-file')\n",
    "\n",
    "x2=torch.load('x-file')\n",
    "x2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6422e0b2",
   "metadata": {},
   "source": [
    "## 存储list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "96487157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#存储list\n",
    "y=torch.zeros(4)\n",
    "torch.save([x,y],'x-file')\n",
    "x2,y2=torch.load('x-file')\n",
    "(x2,y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f564d4c",
   "metadata": {},
   "source": [
    "## 存储字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9663a438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#存储字典\n",
    "mydict={'x':x,'y':y}\n",
    "torch.save(mydict,'mydict')\n",
    "mydict2=torch.load('mydict')\n",
    "\n",
    "mydict2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540fcc52",
   "metadata": {},
   "source": [
    "## 加载和保存模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "92d6cbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.output = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.output(F.relu(self.hidden(x)))\n",
    "\n",
    "net = MLP()\n",
    "X = torch.randn(size=(2, 20))\n",
    "Y = net(X)\n",
    "\n",
    "#将模型的参数存储为一个叫做”mlp.params\"的文件\n",
    "#把整个MLP的参数存储成一个字典，从名字到后面的根映射\n",
    "torch.save(net.state_dict(),'mlp.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8444c188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (hidden): Linear(in_features=20, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#如何load回来呢\n",
    "#不仅要把这个参数带走，还要把MLP这个定义给带走\n",
    "#\n",
    "clone = MLP()\n",
    "#把参数从磁盘上的文件中读取到网络中\n",
    "clone.load_state_dict(torch.load('mlp.params'))\n",
    "clone.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "60807ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#如何验证这个网络呢\n",
    "#输到克隆的这个模型\n",
    "Y_clone=clone(X)\n",
    "Y_clone==Y\n",
    "#来自同一个类的实例，理论上参数和计算都是一样的，所以两次结果都是一样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aa088e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#至此 实现了模型到磁盘的存取，再读回来的一个操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e017d9f7",
   "metadata": {},
   "source": [
    "# 答疑"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004801e7",
   "metadata": {},
   "source": [
    "+ MLP的层数和每一层的单元数，这个每一层的单元数的设置，就有点像编码器，解码器那样，要逐渐减少，逐渐增多，inputsize->inputsize/2->inputsize/4。\n",
    "+ 主要是靠经验\n",
    "+ forward函数就是那个神经网络架构，根据paper里面的来的\n",
    "+ 初始化主要是使得模型在一开始的时候每一层的输入输出的大小在一个尺度上，不要说越往后越大会越小，炸掉。\n",
    "+ 创建好网络之后torch会按一定规则给参数初始化，这个是什么？查一下\n",
    "+ 几乎找不出来不可导的函数，很多函数不是处处可导，但是我们都是数值运算，碰到不可导点概率很小，或者碰到了随便给个值，0，都是离散的点\n",
    "+ 模型可以类似多元函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab208864",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch14] *",
   "language": "python",
   "name": "conda-env-torch14-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
